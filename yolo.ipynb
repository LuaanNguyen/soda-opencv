{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737516550.633138  573090 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
      "I0000 00:00:1737516550.636445  573090 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
      "W0000 00:00:1737516550.636827  573090 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1737516550.640161  573090 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1737516550.646241  585709 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737516550.659069  585709 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737516550.660089  585715 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737516550.666983  585716 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737516550.668295  585715 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1737516550.668446  585715 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume Mode ON\n",
      "Default distance set: 0.032164906837013646\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Up Gesture Detected\n",
      "Volume Up Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Mode OFF\n",
      "Volume Mode ON\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n",
      "Volume Down Gesture Detected\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m---> 63\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7)\n",
    "\n",
    "# Gesture recognizer setup\n",
    "model_path = 'gesture_recognizer.task'\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Global variables\n",
    "gesture_texts = []\n",
    "volume_mode_active = False\n",
    "volume_toggle_ready = True  \n",
    "\n",
    "def print_result(result, output_image, timestamp_ms):\n",
    "    global gesture_texts\n",
    "    gesture_texts = []\n",
    "    if result.gestures and result.handedness:\n",
    "        for hand, gesture in zip(result.handedness, result.gestures):\n",
    "            gesture_texts.append((\n",
    "                hand[0].category_name,\n",
    "                gesture[0].category_name,\n",
    "                gesture[0].score\n",
    "            ))\n",
    "\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt(\n",
    "        (landmark1.x - landmark2.x) ** 2 +\n",
    "        (landmark1.y - landmark2.y) ** 2 +\n",
    "        (landmark1.z - landmark2.z) ** 2\n",
    "    )\n",
    "\n",
    "def adjust_volume(change):\n",
    "    if change > 0:\n",
    "        os.system(\"osascript -e 'set volume output volume (output volume of (get volume settings) + 5)'\")\n",
    "    elif change < 0:\n",
    "        os.system(\"osascript -e 'set volume output volume (output volume of (get volume settings) - 5)'\")\n",
    "\n",
    "# Default distance threshold\n",
    "default_distance = None\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result\n",
    ")\n",
    "\n",
    "try:\n",
    "    with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        timestamp = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process hands\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(\n",
    "                        frame, landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_draw.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                        mp_draw.DrawingSpec(color=(250, 44, 250), thickness=2)\n",
    "                    )\n",
    "\n",
    "                    # Detect middle finger and thumb pinch gesture to toggle volume mode\n",
    "                    thumb_tip = landmarks.landmark[4]\n",
    "                    middle_tip = landmarks.landmark[12]\n",
    "                    pinch_distance = calculate_distance(thumb_tip, middle_tip)\n",
    "\n",
    "                    if pinch_distance < 0.05 and volume_toggle_ready:  # Threshold for pinch gesture\n",
    "                        volume_mode_active = not volume_mode_active\n",
    "                        mode_status = \"ON\" if volume_mode_active else \"OFF\"\n",
    "                        volume_toggle_ready = False  # Prevent multiple toggles in one pinch\n",
    "                        cv2.putText(frame, f\"Volume Mode {mode_status}\", \n",
    "                                   (50, 100),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                                   (255, 255, 0), 2, cv2.LINE_AA)\n",
    "                        print(f\"Volume Mode {mode_status}\")\n",
    "\n",
    "                    if pinch_distance > 0.1:  # Reset toggle readiness when fingers move apart\n",
    "                        volume_toggle_ready = True\n",
    "\n",
    "                    # If volume mode is active, process thumb and index finger gestures for volume control\n",
    "                    if volume_mode_active:\n",
    "                        index_tip = landmarks.landmark[8]\n",
    "                        thumb_tip = landmarks.landmark[4]\n",
    "                        distance = calculate_distance(thumb_tip, index_tip)\n",
    "\n",
    "                        if default_distance is None:\n",
    "                            default_distance = distance  # Set the initial distance as default\n",
    "                            print(f\"Default distance set: {default_distance}\")\n",
    "                        else:\n",
    "                            if distance < default_distance * 0.8:  # Threshold for volume up\n",
    "                                cv2.putText(frame, \"Volume Up!\", \n",
    "                                           (50, 50),\n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                                           (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                                adjust_volume(1)\n",
    "                                print(\"Volume Up Gesture Detected\")\n",
    "                            elif distance > default_distance * 1.2:  # Threshold for volume down\n",
    "                                cv2.putText(frame, \"Volume Down!\", \n",
    "                                           (50, 50),\n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                                           (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                adjust_volume(-1)\n",
    "                                print(\"Volume Down Gesture Detected\")\n",
    "\n",
    "            # Process gestures\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "            timestamp += 1\n",
    "            recognizer.recognize_async(mp_image, timestamp)\n",
    "\n",
    "            # Draw gesture labels\n",
    "            for idx, (hand, gesture, score) in enumerate(gesture_texts):\n",
    "                cv2.putText(frame, f\"{gesture} ({score:.2f})\", \n",
    "                           (10, 30 + idx * 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                           (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('Hand Gesture Recognition', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    if 'cap' in locals():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
